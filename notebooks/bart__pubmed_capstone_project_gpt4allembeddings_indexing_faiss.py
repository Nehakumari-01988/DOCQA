# -*- coding: utf-8 -*-
"""bart_ Pubmed-capstone_Project_GPT4AllEmbeddings_Indexing-Faiss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HGL9oXssxcbRT0ZMG1Q22I5gznLcruLE

## Introduction

In this tutorial, you learn how to use Google Cloud AI tools to quickly bring the power of Large Language Models to enterprise systems.  

This tutorial covers the following -

*   What are embeddings - what business challenges do they help solve ?
*   Understanding Text with Vertex AI Text Embeddings
*   Find Embeddings fast with Vertex AI Vector Search
*   Grounding LLM outputs with Vector Search

This tutorial is based on [the blog post](https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-with-text-embeddings), combined with sample code.


### Prerequisites

This tutorial is designed for developers who has basic knowledge and experience with Python programming and machine learning.

If you are not reading this tutorial in Qwiklab, then you need to have a Google Cloud project that is linked to a billing account to run this. Please go through [this document](https://cloud.google.com/vertex-ai/docs/start/cloud-environment) to create a project and setup a billing account for it.

### Choose the runtime environment

The notebook can be run on either Google Colab or [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).

- To use Colab: Click [this link](https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro-textemb-vectorsearch.ipynb) to open the tutorial in Colab.

- To use Workbench: If it is the first time to use Workbench in your Google Cloud project, open [the Workbench console](https://console.cloud.google.com/vertex-ai/workbench) and click ENABLE button to enable Notebooks API. Then click [this link](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/embeddings/intro-textemb-vectorsearch.ipynb),  and select an existing notebook or create a new notebook.

### How much will this cost?

In case you are using your own Cloud project, not a temporary project on Qwiklab, you need to spend roughly a few US dollars to finish this tutorial.

The pricing of the Cloud services we will use in this tutorial are avilable in the following pages:

- [Vertex AI Embeddings for Text](https://cloud.google.com/vertex-ai/pricing#generative_ai_models)
- [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/pricing#matchingengine)
- [BigQuery](https://cloud.google.com/bigquery/pricing)
- [Cloud Storage](https://cloud.google.com/storage/pricing)
- [Vertex AI Workbench](https://cloud.google.com/vertex-ai/pricing#notebooks) if you use one

You can use the [Pricing Calculator](https://cloud.google.com/products/calculator) to generate a cost estimate based on your projected usage. The following is an example of rough cost estimation with the calculator, assuming you will go through this tutorial a couple of time.

<img src="https://storage.googleapis.com/github-repo/img/embeddings/vs-quickstart/pricing.png" width="50%"/>

### **Warning: delete your objects after the tutorial**

In case you are using your own Cloud project, please make sure to delete all the Indexes, Index Endpoints and Cloud Storage buckets (and the Workbench instance if you use one) after finishing this tutorial. Otherwise the remaining assets would incur unexpected costs.

# Bringing Gen AI and LLMs to production services

Many people are now starting to think about how to bring Gen AI and LLMs to production services, and facing with several challenges.

- "How to integrate LLMs or AI chatbots with existing IT systems, databases and business data?"
- "We have thousands of products. How can I let LLM memorize them all precisely?"
- "How to handle the hallucination issues in AI chatbots to build a reliable service?"

Here is a quick solution: **grounding** with **embeddings** and **vector search**.

What is grounding? What are embedding and vector search? In this tutorial, we will learn these crucial concepts to build reliable Gen AI services for enterprise use. But before we dive deeper, let's try the demo below.

# Vertex AI Embeddings for Text

With the [Vertex AI Embeddings for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings), you can easily create a text embedding with LLM. The product is also available on [Vertex AI Model Garden](https://cloud.google.com/model-garden)

![](https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/7.png)

This API is designed to extract embeddings from texts. It can take text input up to 3,072 input tokens, and outputs 768 dimensional text embeddings.

# Text Embeddings in Action
## Setup

Before get started with the Vertex AI services, we need to setup the following.

* Install Python SDK
* Environment variables
* Authentication (Colab only)
* Enable APIs
* Set IAM permissions

### Install Python SDK
"""

# Install Vertex AI LLM SDK
! pip install --user --upgrade google-cloud-aiplatform==1.47.0 langchain==0.1.14 langchain-google-vertexai==0.1.3 typing_extensions==4.9.0

# Dependencies required by Unstructured PDF loader
#! sudo apt -y -qq install tesseract-ocr libtesseract-dev
#! sudo apt-get -y -qq install poppler-utils
#! pip install --user --upgrade unstructured==0.12.4 pdf2image==1.17.0 pytesseract==0.3.10 pdfminer.six==20221105
#! pip install --user --upgrade pillow-heif==0.15.0 opencv-python==4.9.0.80 unstructured-inference==0.7.24 pikepdf==8.13.0 pypdf==4.0.1

# For Matching Engine integration dependencies (default embeddings)
! pip install --user --upgrade tensorflow_hub==0.16.1 tensorflow_text==2.15.0
! pip install sentence-transformers
! pip install -U langchain-community faiss-gpu
! pip install --upgrade --quiet  sentence_transformers > /dev/null
! pip install langchain_community
! pip install gpt4all

import langchain

print(f"LangChain version: {langchain.__version__}")

"""# Download custom Python modules and utilities
The cell below will download some helper functions needed for using Vertex AI Matching Engine in this notebook. These helper functions were created to keep this notebook more tidy and concise, and you can also view them directly on Github.
"""

# # Automatically restart kernel after installs so that your environment can access the new packages
# import IPython

# app = IPython.Application.instance()
# app.kernel.do_shutdown(True)

#Authenticating your notebook environment
import sys

if "google.colab" in sys.modules:
    from google.colab import auth

    auth.authenticate_user()

PROJECT_ID = "iisccapstone-420805"

"""# Enable APIs
Run the following to enable APIs for Compute Engine, Vertex AI, Cloud Storage and BigQuery with this Google Cloud project.
"""

! gcloud services enable compute.googleapis.com aiplatform.googleapis.com storage.googleapis.com bigquery.googleapis.com --project {PROJECT_ID}

"""### Set IAM permissions

Also, we need to add access permissions to the default service account for using those services.

- Go to [the IAM page](https://console.cloud.google.com/iam-admin/) in the Console
- Look for the principal for default compute service account. It should look like: `<project-number>-compute@developer.gserviceaccount.com`
- Click the edit button at right and click `ADD ANOTHER ROLE` to add `Vertex AI User`, `BigQuery User` and `Storage Admin` to the account.

This will look like this:

![](https://storage.googleapis.com/github-repo/img/embeddings/vs-quickstart/iam-setting.png)

# Environment variables
"""

# get project ID
PROJECT_ID = ! gcloud config get project
PROJECT_ID = "iisccapstone-420805"
LOCATION = "us-central1"
if PROJECT_ID == "(unset)":
    print(f"Please set the project ID manually below")
    # define project information
if PROJECT_ID == "(unset)":
    PROJECT_ID = "iisccapstone-420805'"  # @param {type:"string"}

# generate an unique id for this session
from datetime import datetime

UID = datetime.now().strftime("%m%d%H%M")

PROJECT_ID

"""# Import libraries"""

import vertexai

#PROJECT_ID = PROJECT_ID # @param {type:"string"}
REGION = "us-central1"

vertexai.init(project={PROJECT_ID}, location=REGION)

pip install faiss-gpu

import json
import textwrap

# Utils
import time
import uuid
from typing import List

import numpy as np
import vertexai

# Vertex AI
from google.cloud import aiplatform

print(f"Vertex AI SDK version: {aiplatform.__version__}")

# LangChain
import langchain

print(f"LangChain version: {langchain.__version__}")

from langchain.chains import RetrievalQA
from langchain.document_loaders import GCSDirectoryLoader
from langchain.embeddings import VertexAIEmbeddings
from langchain.llms import VertexAI
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Import custom Matching Engine packages
# from utils.matching_engine import MatchingEngine
# from utils.matching_engine_utils import MatchingEngineUtils
# Import custom Matching Engine packages
from langchain_google_vertexai import VertexAI , VertexAIEmbeddings , VectorSearchVectorStore
import faiss
from faiss import IndexFlatL2
import numpy as np
import spacy
from langchain_community.embeddings import HuggingFaceEmbeddings
import json
#import pdfplumber
from langchain_community.vectorstores import FAISS
import os
from google.colab import files
import zipfile
from langchain.document_loaders import BigQueryLoader #Class for storing a piece of text and associated metadata.
from langchain_community.embeddings import GPT4AllEmbeddings



"""# connecting to bigquery to extract the text data"""

# # load the BQ Table into a Pandas Dataframe
# import pandas as pd
# from google.cloud import bigquery


# bq_client = bigquery.Client(project=PROJECT_ID)
# QUERY_TEMPLATE = """
#         SELECT * from iisccapstone-420805.Pubmed.pubmed where content !='';
#         """
# # query_params=[
# #         bigquery.ArrayQueryParameter("q1","DATE", q1),
# #         bigquery.ArrayQueryParameter("q2","DATE", q2),
# #         bigquery.ArrayQueryParameter("q3","DATE", q3),
# #         bigquery.ArrayQueryParameter("q4","DATE", q4),
# #         bigquery.ArrayQueryParameter("rule_name","STRING", rule_name),
# #         bigquery.ArrayQueryParameter("insert_timestamp","DATE", insert_timestamp),
# #         bigquery.ArrayQueryParameter("Manufacturer","STRING", Manufacturer),
# #         bigquery.ArrayQueryParameter("partner_code","STRING", partner_code),
#     # ]
# try:
#   pubmed = bq_client.query(QUERY_TEMPLATE)  # Make an API request.
#   pubmed_data = pubmed.to_dataframe()
# except Exception as e:
#   print('Error',e,'Data_not_found')

# # examine the data
# pubmed_data.head()

"""# Load the text embeddings model
from vertexai.preview.language_models import TextEmbeddingModel

model = TextEmbeddingModel.from_pretrained("textembedding-gecko@001")

# Load the text embeddings model
"""

# from langchain_google_vertexai import VertexAIEmbeddings

# model = VertexAIEmbeddings(model_name="textembedding-gecko@003")

"""# Load the biquery where data for llm is stored"""

# BASE_QUERY = """SELECT * FROM `iisccapstone-420805.Pubmed.pubmed` where content !=''"""
# loader = BigQueryLoader(BASE_QUERY,project="iisccapstone-420805")
# documents = loader.load()

# Add document metadata to all the document of documents and formatting page_content to only contain contents of BQ table pubmed as it contained both title and content
# for document in documents:
#   document.metadata={'source':document.page_content.split('\n')[0]}
#   document.page_content=document.page_content.split('\n')[1]

# documents[0].page_content

"""# Chunk documents
Split the documents to smaller chunks. When splitting the document, ensure a few chunks can fit within the context length of LLM.
"""

# # split the documents into chunks
# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size=1000,
#     chunk_overlap=50,
#     separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
# )
# doc_splits = text_splitter.split_documents(documents)

# # Add chunk number to metadata
# for idx, split in enumerate(doc_splits):
#     split.metadata["chunk"] = idx

# print(f"# of documents = {len(doc_splits)}")

# doc_splits[0]

"""# creating faiss db to store embeddings in offline mode

"""

# import faiss
# from google.cloud import aiplatform
# from vertexai.language_models import TextEmbeddingModel
# import vertexai

# #PROJECT_ID = PROJECT_ID # @param {type:"string"}
# REGION = "us-central1"
# PROJECT_ID = "iisccapstone-420805"
# vertexai.init(project={PROJECT_ID}, location=REGION)
# text_embedding_model = TextEmbeddingModel.from_pretrained("textembedding-gecko@003")

# db = FAISS.from_documents(doc_splits , GPT4AllEmbeddings())

# print(db.index.ntotal)

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

query = 'prolong alcohol intake impact human health?'
# docs = db.similarity_search(query)

# db.save_local("/content/drive/MyDrive/Capstone_project/GPT4AllEmbeddings/faiss_index")

from pathlib import Path

new_db = FAISS.load_local("/content/drive/MyDrive/Capstone_project/GPT4_FAISS", GPT4AllEmbeddings(),allow_dangerous_deserialization=True)
query = 'prolong alcohol intake impact human health?'

docs = new_db.similarity_search(query)

docs = new_db.similarity_search(query)

new_db.similarity_search_with_score(query)

docs

"""# Model LLaMA2"""

len(docs)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

type(docs)

# converting it into list of string to generate summary
docs1=format_docs(docs)

from transformers import pipeline
model_name="gpt2"
chat_pipeline=pipeline("text-generation",model=model_name)

from transformers import GPT2LMHeadModel, GPT2Tokenizer
from langchain.prompts import PromptTemplate

model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

chat_pipeline(docs1,max_length=1000)

"""# Text generator"""

def generate_text(prompt, max_length=1000):
  input_ids = tokenizer.encode(prompt, return_tensors="pt")
  output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=0.7)
  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
  return generated_text

generate_text(docs1)

"""# Text summarizer"""

from langchain.callbacks.base import BaseCallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.llms import GPT4All
from transformers import BartForConditionalGeneration,BartTokenizer

distilbert-base-uncased-squad2

deepset/roberta-base-squad2
allegro/xlnet-base-cased-multitask

from langchain.callbacks.base import BaseCallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from langchain import PromptTemplate, LLMChain
callback_manager = BaseCallbackManager([StreamingStdOutCallbackHandler()])
llm = GPT4All('orca-mini-3b-gguf2-q4_0.gguf')
llm.generate("Alchol is bad")

def generate_text(prompt, max_length=1000):
  input_ids = tokenizer.encode(prompt, return_tensors="pt")
  output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=0.7)
  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
  return generated_text

template_formatted

type(docs1),template

#end prompt
from transformers import AutoTokenizer, AutoModelForCausalLM
checkpoint = "openai-community/gpt2-large"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)
prompt = "prolong alcohol intake impact human health?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)
tokenizer.batch_decode(outputs, skip_special_tokens=True)

from transformers import pipeline

qa_model = pipeline("question-answering", "facebook/bart-large-cnn")
question = query
context = docs1
qa_model(question = question, context = context)
# {'score': 0.975547730922699, 'start': 28, 'end': 36, 'answer': ' Sweden.'}

"""Bart Model for Summarization"""

from transformers import BartForConditionalGeneration,BartTokenizer
#bart
def text_summarizer(text):
    model_name = "facebook/bart-large-cnn"
    model = BartForConditionalGeneration.from_pretrained(model_name)
    tokenizer = BartTokenizer.from_pretrained(model_name)

    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(inputs, max_length=300, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True,temperature=0.4,do_sample='True')

    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    #formatted_summary = "\n".join(textwrap.wrap(summary, width=80))
    return summary

text_summarizer(template_formatted)

query2 = 'What is an e-cigarrette??'
docs2 = new_db.similarity_search(query2)  #using Faiss
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
docs2=format_docs(docs2)
text_summarizer(docs2)    #using bart

def summarize_text(text, max_length=1000):
  input_ids = tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)
  output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=0.2, early_stopping=True)
  summarized_text = tokenizer.decode(output[0], skip_special_tokens=True)
  return summarize_text

from transformers import pipeline

def summarize_text(text, max_length=1000):
    """Summarize input text using a pre-trained GPT-2 model."""
    summarization_pipeline = pipeline("summarization", model="gpt2")
    summary = summarization_pipeline(text, max_length=max_length, min_length=50, do_sample=True)[0]['summary_text']
    return summary

# Example usage:
input_text = docs1
summary = summarize_text(input_text)
print("Summary:", summary)

"""# Creating db as Retreiver"""

# Create a retriever object from the 'db' using the 'as_retriever' method.
# This retriever is likely used for retrieving data or documents from the database.
retriever = new_db.as_retriever()
docs = retriever.invoke('prolong alcohol intake impact human health?')
docs

docs = retriever.get_relevant_documents("What is prolong alcohol intake impact human health?")

context_summ=' '.join([str(i.page_content) for i in docs])

context_summ

docs = retriever.get_relevant_documents("What is e-cigarrette?")

context_summ1=' '.join([str(i.page_content) for i in docs])

context_summ1

# import openai

# # Set up your OpenAI API key
# openai.api_key = 'sk-proj-dHFkSQp3XoUHkhJQ1MIcT3BlbkFJIQiX7fhFeAGtyYpMDGjV'

# Example text
input_text = context_summ1

!pip show faiss-gpu

!python --version